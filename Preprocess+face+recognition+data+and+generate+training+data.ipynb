{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition Image Cropping and Filtering notebook\n",
    "\n",
    "This note books is program that preprocesing data for face recognition\n",
    "It reads image data from GCS, the images are \n",
    "\n",
    "* filtered : it read data and filter out not appropriate image for example, if it has more than 2 face, if face has sunglass or if face angle is so big. \n",
    "* croped : after filtering it crops face only\n",
    "* resize : croped image will be resized for traing\n",
    "* stored in destination GCS bucket : all processed images are stored in destination bucket in GCS\n",
    "* create labled image list file :for training it also create csv file which combines image URL and label for the image. It generate two csv file. One for training and the other for validation\n",
    "\n",
    "If you just put the images in SOURCE_BUCKET in GCS, it will automatically preprocess and generate result.\n",
    "The process is using Apache Beam (aka. google dataflow) to provide scale.\n",
    "\n",
    "Here is reference doc \n",
    "* Jupyter notebook which uses Apache beam for preprocessing. It is really good!! : https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/babyweight/babyweight.ipynb\n",
    "* GCS python client :  http://gcloud-python.readthedocs.io/en/latest/storage-client.html\n",
    "* Beam python example : https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python modules\n",
    "You need to run this in just first time. After u install the modules. you don't need to install again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT = 'terrycho-ml' #Google Project ID\n",
    "SOURCE_BUCKET = 'terrycho-face-rawdata' #GCS SOURCE BUCKET\n",
    "IMAGE_FILE_PREFIX = '' #directory which stores raw images in GCS\n",
    "DESTINATION_BUCKET = 'terrycho-face-trainingdata' #GCS DESTINATION BUCKET, all filtered and cropped image and file list will be stored\n",
    "INPUT_FILE_LOCAL='filelist.csv'\n",
    "INPUT_FILE='gs://'+SOURCE_BUCKET+'/'+INPUT_FILE_LOCAL \n",
    "\n",
    "IMAGE_SIZE = 96,96\n",
    "\n",
    "MAX_ROLL = 20\n",
    "MAX_TILT = 20\n",
    "MAX_PAN = 20\n",
    "\n",
    "DEBUG_MODE=False\n",
    "NUM_OF_DEBUG_DATA=15\n",
    "TRAINING_FILE = \"training.csv\"\n",
    "VALIDATION_FILE = \"validation.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source image file directory structure\n",
    "\n",
    "SOURCE_BUCKET/IMAGE_PREFIX/Jessica/imagefileX.jpg <BR>\n",
    "SOURCE_BUCKET/IMAGE_PREFIX/Jessica/imagefileX.jpg <BR>\n",
    "SOURCE_BUCKET/IMAGE_PREFIX/Jessica/imagefileX.jpg <BR>\n",
    "SOURCE_BUCKET/IMAGE_PREFIX/Brad/imagefileX.jpg <BR>\n",
    "SOURCE_BUCKET/IMAGE_PREFIX/Brad/imagefileX.jpg <BR>\n",
    "\n",
    "#### Options\n",
    "\n",
    "* MAX_ROLL,MAX_TILT,MAX_PAN : face angle, if face of angle in the image exceeds this value, the image will be filtered out\n",
    "* DEBUG_MODE : if it is true, dataflow pipeline will be ran in local and only NUM_OF_DEBUG_DATA will be used for testing purpose only\n",
    "* TRAINING_FILE : final file name which contains cropped image file name & lables for training purpose\n",
    "* VALIDATION_FILE :  final file name which contains cropped image file name & lables for validation purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define temp directory depends on DEBUG_MODE.\n",
    "If it is development mode with local dataflow runner. it will uses local directory as a temp.\n",
    "In production mode, dataflow runner will be ran in cloud , it uses GCS as a temp. But even it still needs a LOCAL_TMP_DIR. \n",
    "Because, in the code, it pulls files from GCS to local disk to resize/crop etc. the LOCAL_TMP_DIR is used for the purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DEBUG_MODE:\n",
    "    TMP_DIR='/tmp/face'\n",
    "    LOCAL_TMP_DIR = '/tmp/'\n",
    "else:\n",
    "    TMP_DIR='gs://'+DESTINATION_BUCKET+'/tmp/'\n",
    "    LOCAL_TMP_DIR = '/tmp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear garbage data\n",
    "\n",
    "clear local directories before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: training.csv-*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm training.csv-*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make raw file list with CSV format\n",
    "\n",
    "Scan files in SOURCE directory and make list of the files with CSV format <BR>\n",
    "The SOURCE directory will have subdirectory and the sub directoy just have image file of one people. (the people name is the subdirectory name)<BR>\n",
    "file format will be \"filename,labelname,label\"<BR>\n",
    "The label is integer, label name is string\n",
    "<p>\n",
    "<B>before run the process, you have to create service account file (json). you can get the file from google cloud console API menu.\n",
    "After you download it, you have to replace \"/Users/terrycho/keys/terrycho-ml.json\" this key file to your downloaded key file. </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] 2016 FW KOLPING 설현 콜핑 아웃도어 광고01.jpg file processing error by 'ascii' codec can't encode characters in position 16-21: ordinal not in range(128) and skiped\n",
      "\n",
      "Found 456 files and create filelist.csv file\n"
     ]
    }
   ],
   "source": [
    "#gcs example https://github.com/salrashid123/gcpsamples#cloud-python\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/terrycho/keys/terrycho-ml.json\"\n",
    "\n",
    "def create_csv():\n",
    "    # get bucket\n",
    "    credentials, project = google.auth.default()\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(SOURCE_BUCKET)\n",
    "    labels = {}\n",
    "    index =  0\n",
    "    cnt = 0\n",
    "\n",
    "    # open file list file\n",
    "    tfile = open(INPUT_FILE_LOCAL,'w')\n",
    "\n",
    "    # read files in directory\n",
    "    blobs = bucket.list_blobs()\n",
    "    for blob in blobs:\n",
    "        uri = blob.name\n",
    "        # if file is jpeg, extract file name and directory name\n",
    "        if(blob.content_type == 'image/jpeg'):\n",
    "            #print uri\n",
    "            try:\n",
    "                e = uri.split('/')\n",
    "                imagefile = e[len(e)-1]\n",
    "                if ',' in imagefile:\n",
    "                    continue\n",
    "                name = str(e[len(e)-2])\n",
    "                try:\n",
    "                    label = labels[name]\n",
    "                except Exception , e:\n",
    "                    label = index\n",
    "                    labels[name] = label\n",
    "                    index = index + 1\n",
    "\n",
    "                tfile.write('%s,%s,%s\\n'%(imagefile,name,label))\n",
    "                cnt = cnt + 1\n",
    "                # if it is debug mode, it extract small number of data for testing purpose\n",
    "                if DEBUG_MODE and cnt > NUM_OF_DEBUG_DATA:\n",
    "                    break\n",
    "            except Exception , ex:\n",
    "                s = str(ex)\n",
    "                print(\"[Error] %s file processing error by %s and skiped\" %(imagefile,str(ex)) )\n",
    "\n",
    "    tfile.close()\n",
    "    print ('\\nFound %s files and create %s file'%(cnt,INPUT_FILE_LOCAL))\n",
    "\n",
    "create_csv()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate raw csv file and upload it into GCS\n",
    "\n",
    "validate generated file and upload the file into SOURCE_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     456 filelist.csv\n",
      "jesi-1zdcfbt.jpg,Alba,0\n",
      "jesi-2dgou36.jpg,Alba,0\n",
      "jesi-2dqn8li.jpg,Alba,0\n",
      "jesi-2drg9ok.jpg,Alba,0\n",
      "jesi-2dv8uth.jpg,Alba,0\n",
      "jesi-2j5i15c.jpg,Alba,0\n",
      "jesi-2ndimages (10).jpeg,Alba,0\n",
      "jesi-2ndimages (17).jpeg,Alba,0\n",
      "jesi-2ndimages (22).jpeg,Alba,0\n",
      "jesi-2ndimages (23).jpeg,Alba,0\n",
      "Copying file://filelist.csv [Content-Type=text/csv]...\n",
      "\\ [1 files][ 14.4 KiB/ 14.4 KiB]                                                \n",
      "Operation completed over 1 objects/14.4 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# chceck generated file\n",
    "\n",
    "!wc -l $INPUT_FILE_LOCAL\n",
    "!head $INPUT_FILE_LOCAL\n",
    "\n",
    "#upload INPUT_FILE to GCS\n",
    "!gsutil cp $INPUT_FILE_LOCAL gs://$SOURCE_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up DESTINATION bucket which will store filtered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/Babel-2.3.4.tar.gz#1496939000485883...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/Bottleneck-1.1.0.tar.gz#1496939012905200...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/Pillow-3.3.1.zip#1496939102212878...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/Pygments-2.2.0.tar.gz#1496939121678474...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/alabaster-0.7.9.tar.gz#1496938969183040...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/appnope-0.1.0.tar.gz#1496938969832049...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/appscript-1.0.1.tar.gz#1496938971667971...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/argcomplete-1.0.0.tar.gz#1496938972614111...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/astroid-1.4.7.tar.gz#1496938978875521...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/astropy-1.2.1.tar.gz#1496938990930987...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/backports.shutil_get_terminal_size-1.0.0.tar.gz#1496939000970309...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/beautifulsoup4-4.5.1.tar.gz#1496939002103245...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/bitarray-0.8.1.tar.gz#1496939002653779...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/blaze-0.10.1.tar.gz#1496939007441138...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/bokeh-0.12.2.zip#1496939010279143...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/boto-2.42.0.tar.gz#1496939011729578...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/cachetools-2.0.0.tar.gz#1496939016672734...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/certifi-2017.4.17.tar.gz#1496939018523067...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/cffi-1.7.0.tar.gz#1496939021138712...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/chardet-3.0.3.tar.gz#1496939027377981...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/chardet-3.0.4.tar.gz#1496939029352508...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/chest-0.2.3.tar.gz#1496939029918616...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/click-6.6.tar.gz#1496939030886392...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/cloudpickle-0.2.1.tar.gz#1496939031447701...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/colorama-0.3.7.zip#1496939032028814...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/dataflow_python_sdk.tar#1496939149773437...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/decorator-4.0.11.tar.gz#1496939038034334...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/dill-0.2.6.zip#1496939039593660...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/enum34-1.1.6.tar.gz#1496939040330365...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/future-0.16.0.tar.gz#1496939042588250...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/futures-3.1.1.tar.gz#1496939046814443...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/gapic-google-cloud-vision-v1-0.90.3.tar.gz#1496939047561963...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-api-python-client-1.6.2.tar.gz#1496939048394039...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-auth-1.0.1.tar.gz#1496939049765514...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-auth-httplib2-0.0.2.tar.gz#1496939050598703...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-cloud-core-0.24.1.tar.gz#1496939051368204...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-cloud-storage-1.1.1.tar.gz#1496939052239730...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-cloud-vision-0.24.0.tar.gz#1496939057217951...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-gax-0.15.13.tar.gz#1496939058627437...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/google-resumable-media-0.0.2.tar.gz#1496939059508308...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/googleapis-common-protos-1.5.2.tar.gz#1496939060139412...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/grpcio-1.3.5.tar.gz#1496939071678367...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/httplib2-0.10.3.tar.gz#1496939073173224...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/idna-2.5.tar.gz#1496939078064545...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/ipython-5.3.0.tar.gz#1496939082510113...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/ipython_genutils-0.2.0.tar.gz#1496939086797077...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/oauth2client-4.1.1.tar.gz#1496939088281456...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/pathlib2-2.2.1.tar.gz#1496939088882755...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/pexpect-4.2.1.tar.gz#1496939090156224...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/pickled_main_session#1496939139382084...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/pickleshare-0.7.4.tar.gz#1496939090788245...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/ply-3.8.tar.gz#1496939107546197...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/prompt_toolkit-1.0.14.tar.gz#1496939108714784...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/proto-google-cloud-vision-v1-0.90.3.tar.gz#1496939109151243...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/protobuf-3.3.0.tar.gz#1496939110245267...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/ptyprocess-0.5.1.tar.gz#1496939111733090...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/pyasn1-0.2.3.tar.gz#1496939117672570...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/pyasn1-modules-0.0.9.tar.gz#1496939118381947...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/requests-2.17.3.tar.gz#1496939122964134...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/requirements.txt#1496938938303716...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/rsa-3.4.2.tar.gz#1496939126984157...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/scandir-1.5.tar.gz#1496939127652869...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/setuptools-36.0.1.zip#1496939129621795...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/simplegeneric-0.8.1.zip#1496939130171481...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/six-1.10.0.tar.gz#1496939130863834...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/traitlets-4.3.2.tar.gz#1496939132005125...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/uritemplate-3.0.0.tar.gz#1496939132536771...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/urllib3-1.21.1.tar.gz#1496939137976082...\n",
      "Removing gs://terrycho-face-trainingdata/staging/preparefacedata170609012211.1496938933.100437/wcwidth-0.1.7.tar.gz#1496939138739071...\n",
      "Removing gs://terrycho-face-trainingdata/tmp/#1496939161404447...\n",
      "Removing gs://terrycho-face-trainingdata/tmp/preparefacedata170609012211.1496938933.100437/#1496939162012748...\n",
      "Removing gs://terrycho-face-trainingdata/tmp/preparefacedata170609012211.1496938933.100437/dax-tmp-2017-06-08_09_25_51-2992369704772894580-S01-1-d7a2d0f1ffd1ea16/#1496939162618779...\n",
      "Removing gs://terrycho-face-trainingdata/tmp/preparefacedata170609012211.1496938933.100437/dax-tmp-2017-06-08_09_25_51-2992369704772894580-S01-2-d7a2d0f1ffd1e3cf/#1496939163198339...\n",
      "/ [73/73 objects] 100% Done                                                     \n",
      "Operation completed over 73 objects.                                             \n",
      "Removing gs://terrycho-face-trainingdata/...\n",
      "Creating gs://terrycho-face-trainingdata/...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# clear destination bucket\n",
    "!gsutil -m rm -r \"gs://\"$DESTINATION_BUCKET\n",
    "\n",
    "# create destination bucket\n",
    "!gsutil mb \"gs://\"$DESTINATION_BUCKET\n",
    "!echo 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make requirements.txt file for python module dependency\n",
    "\n",
    "To run Apache beam data flow in google cloud, it need to upload dependency files into apache beam cloud run time (aka dataflow). \n",
    "This file will be used to install dependency module to google data flow run time\n",
    "Reference https://cloud.google.com/dataflow/pipelines/dependencies-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "Pillow==3.3.1\n",
    "google-api-python-client==1.6.2\n",
    "google-cloud-vision==0.24.0\n",
    "google-cloud-storage==1.1.1\n",
    "ipython==5.3.0\n",
    "ipython-genutils==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow==3.3.1\r\n",
      "google-api-python-client==1.6.2\r\n",
      "google-cloud-vision==0.24.0\r\n",
      "google-cloud-storage==1.1.1\r\n",
      "ipython==5.3.0\r\n",
      "ipython-genutils==0.2.0"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run filtering & resize image file and make a list of filtered file\n",
    "\n",
    "This is Apache beam based data preprocessing workflow\n",
    "I read $INPUT_FILE the file is csv format file and it contains imagefilename, string label and int label.<BR>\n",
    "based on the csv file, it read imagefile and filter the file based on\n",
    "number of faces in the photo, face angle, sunglass etc.<BR>\n",
    "After the filtering it crop & resize the image into IMAGE_SIZE*IMAGE_SIZE <BR>\n",
    "All filtered and cropped image will be uploaded destination bucket. <BR>\n",
    "The file list also stored in destination bucket. <BR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"oauth2client.contrib.multistore_file\"\n",
      "/Users/terrycho/anaconda/envs/tensorflow1.0/lib/python2.7/site-packages/apache_beam/io/gcp/gcsio.py:113: DeprecationWarning: object() takes no parameters\n",
      "  super(GcsIO, cls).__new__(cls, storage_client))\n",
      "/Users/terrycho/anaconda/envs/tensorflow1.0/lib/python2.7/site-packages/apache_beam/coders/typecoders.py:135: UserWarning: Using fallback coder for typehint: Any.\n",
      "  warnings.warn('Using fallback coder for typehint: %r.' % typehint)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "import google.auth\n",
    "import io\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# set service account file into OS environment value\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/terrycho/keys/terrycho-ml.json\"\n",
    "\n",
    "# make variables in option\n",
    "\n",
    "job_name = 'preparefacedata'+ datetime.datetime.now().strftime('%y%m%d%H%M%S')\n",
    "\n",
    "options = {\n",
    "    'staging_location': 'gs://'+DESTINATION_BUCKET+'/staging',\n",
    "    'temp_location': 'gs://'+DESTINATION_BUCKET+'/tmp',\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True ,  \n",
    "    'requirements_file' : 'requirements.txt',\n",
    "    'save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "# define transform\n",
    "\n",
    "def parseCSV(element):\n",
    "    line = str(element)\n",
    "    print line\n",
    "    e = line.split(',')\n",
    "    imagefile = str(e[0])\n",
    "    name = str(e[1])\n",
    "    label = int(e[2])\n",
    "    return label,name,imagefile\n",
    "\n",
    "# filter image with image information with vision API\n",
    "def get_image_info(element):\n",
    "    # in dataflow, globally imported module cannot be used. so import the module in each function\n",
    "    # for more information https://cloud.google.com/dataflow/faq\n",
    "    from google.cloud import vision\n",
    "    if len(element) < 3 or element == None:\n",
    "         print('[Error] %s: It doesnt have file name' % element)\n",
    "         return None\n",
    "    label = int(element[0])\n",
    "    name = str(element[1])\n",
    "    imagefile = str(element[2])\n",
    "    \n",
    "    visionClient = vision.Client()\n",
    "    imagefile_uri = 'gs://'+SOURCE_BUCKET+'/'+IMAGE_FILE_PREFIX+name+'/'+imagefile\n",
    "    print ('[INFO] processing %s'%(imagefile_uri))\n",
    "    image = visionClient.image(source_uri=imagefile_uri)\n",
    "    faces = image.detect_faces(limit=2)\n",
    "\n",
    "    if len(faces) > 1:\n",
    "        print('[Error] %s: It has more than 2 faces in a file' % imagefile)\n",
    "        return None\n",
    "    if len(faces) == 0:\n",
    "        print('[Error] %s: It has no faces in a file' % imagefile)\n",
    "        return None\n",
    "    face = faces[0]\n",
    "\n",
    "    # extract face angle\n",
    "    roll_angle = face.angles.roll\n",
    "    pan_angle = face.angles.pan\n",
    "    tilt_angle = face.angles.tilt\n",
    "    angle = [roll_angle,pan_angle,tilt_angle]\n",
    "    \n",
    "    # filter out based on angle\n",
    "    if abs(roll_angle) > MAX_ROLL or abs(pan_angle) > MAX_PAN or abs(tilt_angle) > MAX_TILT:\n",
    "        print('[Error] %s: face skew angle is big' % imagefile)\n",
    "        return None\n",
    "        \n",
    "    # extract face boundary\n",
    "    left = face.fd_bounds.vertices[0].x_coordinate\n",
    "    top = face.fd_bounds.vertices[0].y_coordinate\n",
    "    right = face.fd_bounds.vertices[2].x_coordinate\n",
    "    bottom = face.fd_bounds.vertices[2].y_coordinate\n",
    "    rect = [left,top,right,bottom]\n",
    "    \n",
    "    # check sunglass\n",
    "    try:\n",
    "        objs = image.detect_labels(limit=50)\n",
    "        if objs != None:\n",
    "            for obj in objs:\n",
    "                if 'sunglasses' in obj.description:\n",
    "                    print('[Error] %s: sunglass is detected' % imagefile)  \n",
    "                    return None\n",
    "    except Exception as e:\n",
    "        print('[Error] %s: Get Label info error: %s' %(imagefile,str(e)) )\n",
    "        return None\n",
    "    \n",
    "    return label,name,imagefile,rect\n",
    "\n",
    "def process_image(element):\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    from PIL import Image\n",
    "    from PIL import ImageDraw\n",
    "    \n",
    "    print \"process image\",element\n",
    "\n",
    "    if element == None  or len(element) < 4:\n",
    "        print('[Error] $s doesnt have 4 elements '%(str(element)) )\n",
    "        return None\n",
    "    \n",
    "    label = int(element[0])\n",
    "    name = str(element[1])\n",
    "    imagefile = str(element[2])\n",
    "    rect = element[3]\n",
    "    print ('[INFO] Cropping %s'%(imagefile))\n",
    "\n",
    "    # crop filesfile\n",
    "    storageClient = storage.Client()\n",
    "    source_bucket = storageClient.get_bucket(SOURCE_BUCKET)\n",
    "    blob = source_bucket.get_blob(IMAGE_FILE_PREFIX+name+'/'+imagefile)\n",
    "    \n",
    "    # 1) download file\n",
    "    tmp_file = LOCAL_TMP_DIR+'tmp-'+imagefile\n",
    "    cropped_file = LOCAL_TMP_DIR+imagefile\n",
    "    with open(tmp_file,'wb') as file_obj:\n",
    "        blob.download_to_file(file_obj)\n",
    "        \n",
    "    # 2) crop face\n",
    "    try:\n",
    "        fd = io.open(tmp_file,'rb')\n",
    "        image = Image.open(fd)  \n",
    "\n",
    "        crop = image.crop(rect)\n",
    "        im = crop.resize(IMAGE_SIZE,Image.ANTIALIAS)\n",
    "            \n",
    "        im.save(cropped_file,\"JPEG\")\n",
    "        fd.close()\n",
    "        print('[Info]  %s: Crop face %s and write it to file : %s' %( imagefile,rect,cropped_file) )\n",
    "\n",
    "    except Exception as e:\n",
    "        print('[Error] %s: Crop image writing error : %s' %(imagefile,str(e)) )\n",
    "        return None\n",
    "    \n",
    "    # 3) upload file\n",
    "    destination_bucket = storageClient.get_bucket(DESTINATION_BUCKET)\n",
    "    \n",
    "    blob = Blob('images/'+imagefile,destination_bucket)\n",
    "    with open(cropped_file,'rb') as file_obj:\n",
    "        blob.upload_from_file(file_obj)\n",
    "        \n",
    "    # 4) delete file\n",
    "    os.remove(cropped_file)\n",
    "    os.remove(tmp_file)\n",
    "    return imagefile,name,label\n",
    "\n",
    "def FormatString(element):\n",
    "    if element == None:\n",
    "        return\n",
    "\n",
    "    s=''\n",
    "    try:\n",
    "        s = '%s,%s,%s'%(element[0],element[1],element[2])\n",
    "    except Exception as e:\n",
    "        print('[Error] %s: file formating error : %s' %(str(element),str(e))) \n",
    "    print s\n",
    "    return s\n",
    "\n",
    "    \n",
    "# create pipeline\n",
    "\n",
    "\n",
    "\n",
    "def run():    \n",
    "    if(DEBUG_MODE):\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "\n",
    "    # Extract image data --> Filter --> Resize & upload --> create file list \n",
    "    l=(p \n",
    "     | 'read csv' >> ReadFromText(INPUT_FILE)\n",
    "     | 'parse CSV file' >> beam.Map(parseCSV)\n",
    "     | 'Get image meta info' >> beam.Map(get_image_info)\n",
    "     | 'Filter unsuitable image' >> beam.Filter(lambda x: x!=None)\n",
    "     | 'Process Image' >> beam.Map(process_image) \n",
    "     | 'Format String' >> beam.Map(FormatString)\n",
    "     | 'Write to training file' >> WriteToText('gs://'+DESTINATION_BUCKET+'/'+TRAINING_FILE)\n",
    "    )\n",
    "    job = p.run()\n",
    "    job.wait_until_finish()\n",
    "\n",
    "run()\n",
    "print 'file filtering and cropping is done!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait until the dataflow job has been finished.\n",
    "If you run the data flow in google cloud, you can check the progress in google cloud console.\n",
    "In my case, when i process 456 files, it takes around 17 min. \n",
    "In the console you can trace applicatoin log and data flow status like below\n",
    "\n",
    "!daatf\n",
    "\n",
    "# Read cropped file list and seperate it to Training & Validation file\n",
    "\n",
    "the filtered and cropped file need to be seperated to two type of data. One is for training and the other is for validation. <P>\n",
    "To do that, it donwload all filtered and croped file from destination bucket and merge it into one file.<BR>\n",
    "After that, for each label, 70% of imagefile are stored in training file list and 30% of image files are stored in validation file\n",
    "\n",
    "### Download cropped files from CSV and merge the lists into filtered_filelist.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://terrycho-face-trainingdata/training.csv-00000-of-00013...\n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00001-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00002-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00003-of-00013...          \n",
      "/ [4 files][  6.3 KiB/  6.3 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m -o ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00004-of-00013...\n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00005-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00006-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00007-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00008-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00009-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00010-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00011-of-00013...          \n",
      "Copying gs://terrycho-face-trainingdata/training.csv-00012-of-00013...          \n",
      "- [13 files][ 10.5 KiB/ 10.5 KiB]                                               \n",
      "Operation completed over 13 objects/10.5 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://$DESTINATION_BUCKET/*.csv* .\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm filtered_filelist.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     318 filtered_filelist.csv\n",
      "Alba\n",
      "      67\n",
      "Sulhyun\n",
      "      73\n",
      "Jolie\n",
      "      69\n",
      "Victoria\n",
      "      54\n",
      "Nicole\n",
      "      55\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "#!ls *.csv*\n",
    "\n",
    "!cat training.csv-* > filtered_filelist.csv\n",
    "!wc -l filtered_filelist.csv\n",
    "\n",
    "#!cat filtered_filelist.csv\n",
    "!echo 'Alba'\n",
    "!egrep Alba filtered_filelist.csv | wc -l\n",
    "!echo 'Sulhyun'\n",
    "!egrep Sulhyun filtered_filelist.csv | wc -l\n",
    "!echo 'Jolie'\n",
    "!egrep Jolie filtered_filelist.csv | wc -l\n",
    "!echo 'Victoria'\n",
    "!egrep Victoria filtered_filelist.csv | wc -l\n",
    "!echo 'Nicole'\n",
    "!egrep Nicole filtered_filelist.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate 70% files to  training file and 30% files to validation file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split is done\n",
      "Alba totla: 66 training data : 52 validation data : 19\n",
      "Sulhyun totla: 72 training data : 57 validation data : 21\n",
      "Jolie totla: 68 training data : 54 validation data : 20\n",
      "Nicole totla: 54 training data : 43 validation data : 16\n",
      "Victoria totla: 53 training data : 42 validation data : 15\n",
      "{'Alba': 66, 'Sulhyun': 72, 'Jolie': 68, 'Nicole': 54, 'Victoria': 53}\n"
     ]
    }
   ],
   "source": [
    "MAX_DATA=50\n",
    "labels = {}\n",
    "\n",
    "flist = open('filtered_filelist.csv','r')\n",
    "tfile = open(TRAINING_FILE,'w')\n",
    "vfile = open(VALIDATION_FILE,'w')\n",
    "\n",
    "for line in flist:\n",
    "    e = line.split(',')\n",
    "    imagefile = e[0]\n",
    "    name = e[1]\n",
    "    label = e[2]\n",
    "    \n",
    "    if name in labels:\n",
    "        labels[name] = labels[name] + 1\n",
    "    else:\n",
    "        labels[name] = 0\n",
    "    \n",
    "    if labels[name] < int(0.8*MAX_DATA):\n",
    "        tfile.write(line)\n",
    "    else:\n",
    "        vfile.write(line)\n",
    "\n",
    "print \"Data split is done\"\n",
    "for l in labels:\n",
    "    print l,'totla:',labels[l],'training data :',int(labels[l]*0.8),'validation data :',int(labels[l]*0.3)\n",
    "\n",
    "\n",
    "flist.close()\n",
    "tfile.close()\n",
    "vfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     175 training.csv\n",
      "     143 validation.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l $TRAINING_FILE\n",
    "!wc -l $VALIDATION_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the training file and validation file into destination file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://training.csv [Content-Type=text/csv]...\n",
      "- [1 files][  6.0 KiB/  6.0 KiB]                                                \n",
      "Operation completed over 1 objects/6.0 KiB.                                      \n",
      "Copying file://validation.csv [Content-Type=text/csv]...\n",
      "- [1 files][  4.5 KiB/  4.5 KiB]                                                \n",
      "Operation completed over 1 objects/4.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $TRAINING_FILE gs://$DESTINATION_BUCKET/\n",
    "!gsutil cp $VALIDATION_FILE gs://$DESTINATION_BUCKET/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file is gs://terrycho-face-trainingdata/training.csv\n",
      "Validation file is gs://terrycho-face-trainingdata/training.csv\n"
     ]
    }
   ],
   "source": [
    "!echo 'Training file is gs://'$DESTINATION_BUCKET'/'$TRAINING_FILE\n",
    "!echo 'Validation file is gs://'$DESTINATION_BUCKET'/'$TRAINING_FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Image preprocessing is done. Next time i will introduce face recognition model that uses this filtered data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
